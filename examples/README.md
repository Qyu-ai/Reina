## About Example Notebooks

The notebooks assume a test_data.csv generated by the following code snippet (Python):    

    import pandas as pd
    df = pd.DataFrame(np.random.rand(1000, 7))  # Sample size is arbitrary. Could increase size if you'd like to try toy big-data.
    df.columns = ['var1', 'var2', 'var3', 'var4', 'var5', 'treatment', 'outcome']
    df.treatment = np.random.randint(2, size=df.shape[0])
    df.to_csv("test_data.csv")

The purpose of these notebooks is just to demonstrate how to get started with our package. For simplicity, the data is small (1000 samples) and is randomly generated. Hence, the treatment effects produced in the scripts have no meaning. If you'd like to try out big-data processing, please increase the sample size to 50M+.

Demo with actual datasets and meaningful effects will come soon.

## Running on AWS

You could setup a Spark cluster easily through the AWS EMR (Elastic Map Reduce) and AWS S3 services. 

Once you setup a Spark cluster using EMR, ssh into the master node. Copy your scripts from S3 using 
    
    aws s3 cp s3://your-bucket/your-file .

Use *sudo* if you run into authorization issues. Once you have the scripts, change the line which reads in the data to point to the location of your data on S3.
 


