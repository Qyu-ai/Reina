{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIvb0JodYx9d"
   },
   "source": [
    "#### Import reina and other necessary libraries. Initialize a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install reina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9mGaEKdeU89A"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reina'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8171cc90edac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mreina\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetalearners\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSLearner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreina\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetalearners\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTLearner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreina\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetalearners\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXLearner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'reina'"
     ]
    }
   ],
   "source": [
    "from reina.metalearners import SLearner\n",
    "from reina.metalearners import TLearner\n",
    "from reina.metalearners import XLearner\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize spark session\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName('Meta-Learner-Spark') \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNHXEHMOY9PY"
   },
   "source": [
    "#### Read toy data. Replace .load() with the test_data.csv location -- this location could be a local one (no cluster) or it could be on a distributed storage system (e.g., HDFS)\n",
    "\n",
    "*Note: Code below assumes data generated by our script (for specifics, please refer to our toy data generation in the README). You could also modify the code accordingly to use your own data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZyUrMTBVJAf"
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "          .format(\"csv\") \\\n",
    "          .option('header', 'true') \\\n",
    "          .load(\"test_data.csv\")  # replace with the location of test_data.csv\n",
    "\n",
    "# Case variables to appropriate types\n",
    "df = df.withColumn(\"var1\", df.var1.cast(\"float\"))\n",
    "df = df.withColumn(\"var2\", df.var2.cast(\"float\"))\n",
    "df = df.withColumn(\"var3\", df.var3.cast(\"float\"))\n",
    "df = df.withColumn(\"var4\", df.var4.cast(\"float\"))\n",
    "df = df.withColumn(\"var5\", df.var5.cast(\"float\"))\n",
    "df = df.withColumn(\"treatment\", df.treatment.cast(\"float\"))\n",
    "df = df.withColumn(\"outcome\", df.outcome.cast(\"float\"))\n",
    "\n",
    "# Drop garbage column\n",
    "df = df.drop(\"_c0\")\n",
    "\n",
    "# Print out dataframe schema\n",
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Xa3QohmXg2Y"
   },
   "source": [
    "## S-leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkMiSReQW2DE"
   },
   "outputs": [],
   "source": [
    "# Set up necessary parameters\n",
    "treatments = ['treatment']\n",
    "outcome = 'outcome'\n",
    "\n",
    "# Arbitrary estimator. Can replace with other ML algo.\n",
    "estimator = RandomForestRegressor()\n",
    "\n",
    "# Fit S-learner\n",
    "spark_slearner = SLearner()\n",
    "spark_slearner.fit(data=df, treatments=treatments, outcome=outcome, estimator=estimator)\n",
    "\n",
    "# Get heterogeneous treatment effects (cate for individual samples and ate for averaged treatment effect)\n",
    "cate, ate = spark_slearner.effects()\n",
    "print(cate)\n",
    "print(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1fcO5LOXjH_"
   },
   "source": [
    "## T-leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn1QYEJPXWvM"
   },
   "outputs": [],
   "source": [
    "# Set up necessary parameters\n",
    "treatments = ['treatment']\n",
    "outcome = 'outcome'\n",
    "\n",
    "# Arbitrary estimators. Can replace with other ML algo.\n",
    "estimator_1 = RandomForestRegressor()\n",
    "estimator_0 = RandomForestRegressor()\n",
    "\n",
    "# Fit T-learner\n",
    "spark_tlearner = TLearner()\n",
    "spark_tlearner.fit(data=df, treatments=treatments, outcome=outcome,\n",
    "                   estimator_0=estimator_0, estimator_1=estimator_1)\n",
    "\n",
    "# Get heterogeneous treatment effects (cate for individual samples and ate for averaged treatment effect)\n",
    "cate, ate = spark_tlearner.effects()\n",
    "print(cate)\n",
    "print(ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxZQdw1fXoPa"
   },
   "source": [
    "## X-leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj1owlx-XYKo"
   },
   "outputs": [],
   "source": [
    "# Set up necessary parameters\n",
    "treatments = ['treatment']\n",
    "outcome = 'outcome'\n",
    "\n",
    "# Arbitrary estimators. Can replace with other ML algo.\n",
    "estimator_11 = RandomForestRegressor()\n",
    "estimator_10 = RandomForestRegressor()\n",
    "estimator_21 = RandomForestRegressor()\n",
    "estimator_20 = RandomForestRegressor()\n",
    "propensity_estimator = RandomForestClassifier()\n",
    "\n",
    "# Fit X-learner\n",
    "spark_xlearner = XLearner()\n",
    "spark_xlearner.fit(data=df, treatments=treatments, outcome=outcome, \n",
    "                       estimator_10=estimator_10, estimator_11=estimator_11, \n",
    "                       estimator_20=estimator_20, estimator_21=estimator_21,\n",
    "                       propensity_estimator=propensity_estimator)\n",
    "\n",
    "# Get heterogeneous treatment effects (cate for individual samples and ate for averaged treatment effect)\n",
    "cate, ate = spark_xlearner.effects()\n",
    "print(cate)\n",
    "print(ate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
