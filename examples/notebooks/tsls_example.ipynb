{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from reina.IvMethods.TSLS import SieveTSLS\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "\r\n",
        "# Initialize spark session\r\n",
        "spark = SparkSession \\\r\n",
        "            .builder \\\r\n",
        "            .appName('Meta-Learner-Spark') \\\r\n",
        "            .getOrCreate()"
      ],
      "outputs": [],
      "metadata": {
        "id": "K9SvSdnga5Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read toy data. Replace .load() with the test_data.csv location -- this location could be a local one (no cluster) or it could be on a distributed storage system (e.g., HDFS)\n",
        "\n",
        "*Note: Code below assumes data generated by our script (for specifics, please refer to our toy data generation in the README). You could also modify the code accordingly to use your own data.*"
      ],
      "metadata": {
        "id": "tmJ284pVbCll"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df = spark.read \\\r\n",
        "          .format(\"csv\") \\\r\n",
        "          .option('header', 'true') \\\r\n",
        "          .load(\"test_data.csv\")  # replace with the location of test_data.csv\r\n",
        "\r\n",
        "# Case variables to appropriate types\r\n",
        "df = df.withColumn(\"var1\", df.var1.cast(\"float\"))\r\n",
        "df = df.withColumn(\"var2\", df.var2.cast(\"float\"))\r\n",
        "df = df.withColumn(\"var3\", df.var3.cast(\"float\"))\r\n",
        "df = df.withColumn(\"var4\", df.var4.cast(\"float\"))\r\n",
        "df = df.withColumn(\"var5\", df.var5.cast(\"float\"))\r\n",
        "df = df.withColumn(\"treatment\", df.treatment.cast(\"float\"))\r\n",
        "df = df.withColumn(\"outcome\", df.outcome.cast(\"float\"))\r\n",
        "\r\n",
        "# Drop garbage column\r\n",
        "df = df.drop(\"_c0\")\r\n",
        "\r\n",
        "# Print out dataframe schema\r\n",
        "print(df.schema)"
      ],
      "outputs": [],
      "metadata": {
        "id": "nS8iORFRa66J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Two-stage Least Squares"
      ],
      "metadata": {
        "id": "4FBJmTkvbbH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Set up necessary parameters\r\n",
        "treatments = ['treatment']\r\n",
        "outcome = 'outcome'\r\n",
        "iv = 'var1'\r\n",
        "\r\n",
        "# Fit TSLS\r\n",
        "spark_tsls = SieveTSLS()\r\n",
        "spark_tsls.fit(data=df, treatments=treatments, outcome=outcome, iv=iv)\r\n",
        "\r\n",
        "# Get heterogeneous treatment effects (cate for individual samples and ate for averaged treatment effect)\r\n",
        "cate, ate = spark_tsls.effect()\r\n",
        "print(cate)  \r\n",
        "print(ate)"
      ],
      "outputs": [],
      "metadata": {
        "id": "mFP8fQkkbAgx"
      }
    }
  ]
}